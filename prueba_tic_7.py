# -*- coding: utf-8 -*-
"""Prueba TIC 7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MATBKM3IVoS74zMoAqySr9xBw2S4vyW2

# Importaciones
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as pltimg
from pandas.plotting import scatter_matrix
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from matplotlib import pyplot
from sklearn.model_selection import train_test_split
#metricas
from sklearn.metrics import silhouette_score
#from sklearn.metrics import accuracy_score
#from sklearn.metrics import confusion_matrix
#from sklearn.metrics import classification_report
from scipy.spatial.distance import cdist

"""# Análisis de la data"""

df = pd.read_csv('alzheimer.csv')
df

"""### **Variables**
#### **M/F:** Género del sujeto (M para masculino, F para femenino).
#### **Age:** Edad del sujeto en el momento del estudio.
#### **Education:** Nivel educativo del sujeto medido en años de educación.
#### **SES (Socioeconomic Status):** Estatus socioeconómico del sujeto, medido en una escala cualitativa.
#### **MMSE (Mini-Mental State Examination):** Puntuación en una prueba estandarizada que evalúa el estado mental del sujeto. Se utiliza comúnmente para diagnosticar demencia.
#### **CDR (Clinical Dementia Rating):** Escala que mide la severidad de la demencia en el sujeto, desde 0 (sin demencia) hasta 3 (demencia severa).
#### **eTIV (Estimated Total Intracranial Volume):** Volumen total intracraneal estimado, medido en milímetros cúbicos.
#### **nWBV (Normalized Whole Brain Volume):** Volumen total del cerebro normalizado, expresado como un porcentaje del volumen intracraneal total.
#### **ASF (Atlas Scaling Factor):** Factor de escala utilizado para normalizar los volúmenes cerebrales según un atlas de referencia.

### Variable de trabajo
#### Group: Indica la clasificación del sujeto en uno de los tres grupos:

#### Demented: Personas diagnosticadas con demencia.
#### No Demented: Personas diagnosticadas sin demencia.
#### Converted: Personas que inicialmente no tenían demencia, pero que desarrollaron la enfermedad durante el seguimiento.


"""

df.describe()

df.info()

"""# Tratamiento de los datos"""

# Obtener una Serie que indica cuántos valores nulos hay en cada columna
null_counts = df.isnull().sum()

# Filtrar las columnas que tienen al menos un valor nulo
columns_with_nulls = null_counts[null_counts > 0]

# Imprimir la lista de columnas y la cantidad de valores nulos en cada una
for column, count in columns_with_nulls.items():
    print(f"Columna '{column}' tiene {count} valores nulos.")

missing_percentage = df.isnull().mean() * 100
print(missing_percentage)

df = df.fillna(df.mean(numeric_only=True)) #Valores nulos El comando df.fillna(df.mean()) se utiliza para rellenar los valores faltantes (NaN) en un DataFrame df con la media de cada columna correspondiente.

# Obtener una Serie que indica cuántos valores nulos hay en cada columna
null_counts = df.isnull().sum()

# Filtrar las columnas que tienen al menos un valor nulo
columns_with_nulls = null_counts[null_counts > 0]

# Imprimir la lista de columnas y la cantidad de valores nulos en cada una
for column, count in columns_with_nulls.items():
    print(f"Columna '{column}' tiene {count} valores nulos.")

"""### Para realizar el algoritmo PCA, se debe considerar que se deben tener valores numericos debido a que utiliza conceptos de álgebra lineal para los cuales no estan contemplados valores categóricos.

### Para este caso se realizará un "one hot enconding" para la variable categorica M/F
"""

# Lista de columnas categóricas que se quiere codificar
categorical_columns = ['M/F']

# Aplicar One Hot Encoding a las columnas categóricas
df = pd.get_dummies(df, columns=categorical_columns)

print('ONE HOT ENCODING TERMINADO')
print(df.head())

"""# Correlaciones

#### Se realizara una copia del dataframe para una prelimpieza de nuestro dataframe de trabajo, esto se realiza para ver las correlaciones entre las variable con respecto a la variable de trabajo group para lo cual se realiza un labelEncoder en la variable Group para observar de mejor manera las correlaciones.
"""

df2 = df.copy()

encoder = LabelEncoder()
df2['Group'] = encoder.fit_transform(df['Group'])

sns.set(style="whitegrid",font_scale=2.5)
plt.figure(figsize=(30,30))
sns.heatmap(df2.corr(),vmax=0.8,square=True,cmap='GnBu',linecolor='b',annot=True)
plt.title('Indice de correlacion de Pearson')
plt.show()

"""#### Se eliminaran las variables "Age", "SES", "eTIV", "ASF" debido a que en el gráfico de correlacion de pearson muestra que dichas variables no son influyentes dentro de la data para lo cual existe la posibilidad de que generen ruido."""

df = df.drop(columns=['Age'])
df = df.drop(columns=['SES'])
df = df.drop(columns=['eTIV'])
df = df.drop(columns=['ASF'])

df2 = df2.drop(columns=['Age'])
df2 = df2.drop(columns=['SES'])
df2 = df2.drop(columns=['eTIV'])
df2 = df2.drop(columns=['ASF'])

sns.set(style="whitegrid",font_scale=2.5)
plt.figure(figsize=(30,30))
sns.heatmap(df2.corr(),vmax=0.8,square=True,cmap='GnBu',linecolor='b',annot=True)
plt.title('Indice de correlacion de Pearson')
plt.show()

"""## Seleccion de variables"""

set_var=set(df.select_dtypes(exclude='O').columns)
set_var

set_var_list = list(set_var)
train_df = df[set_var_list].copy()

train_df.head()

sns.set(style="whitegrid",font_scale=2.5)
plt.figure(figsize=(30,30))
sns.heatmap(train_df.corr(),vmax=0.8,square=True,cmap='GnBu',linecolor='b',annot=True)
plt.title('Matriz de correlaciones')
plt.show()

"""# Escalado de variables

"""

std=StandardScaler()
std_train=std.fit_transform(train_df)
std_train

"""# Algoritmo PCA

#### Algoritmo PCA (Principal Component Analysis)
#### El Análisis de Componentes Principales (PCA) es un método estadístico utilizado para reducir la dimensionalidad de un conjunto de datos mientras se conserva la mayor cantidad posible de variación presente en el conjunto original. Se utiliza principalmente para simplificar los datos, reducir el ruido y facilitar la visualización y análisis.
"""

pca=PCA().fit(std_train)

ar_varianza=pca.explained_variance_ratio_
ar_varianza

pca.explained_variance_#contienes los valores propios

pca.components_ #contiene los vectores propios

# Crear la gráfica de barras
plt.figure(figsize=(15, 15))
plt.bar(range(1, len(ar_varianza) + 1), ar_varianza, tick_label=range(1, len(ar_varianza) + 1))
plt.title('Varianza explicada por cada componente')
plt.xlabel('N componentes')
plt.ylabel('Varianza explicada')

# Agregar anotaciones con los valores
for i, value in enumerate(ar_varianza):
    plt.annotate(f'{value:.2f}', (i + 1, value), textcoords="offset points", xytext=(0,5), ha='center')

plt.show()

# Crear la gráfica
plt.figure(figsize=(15,15))
cumsum_varianza = np.cumsum(ar_varianza)
plt.plot(range(1, len(cumsum_varianza) + 1), cumsum_varianza, marker='o')
plt.xlabel('N componentes')
plt.ylabel('Suma acumulada de la varianza')

# Agregar anotaciones con los valores
for i, value in enumerate(cumsum_varianza):
    plt.annotate(f'{value:.2f}', (i + 1, value), textcoords="offset points", xytext=(0,5), ha='center')
plt.show()


#plt.plot(np.cumsum(ar_varianza))
#plt.xlabel('N componentes')
#plt.ylabel('Suma acumulada de la varianza')
#plt.show()

ar_Pca_suma=np.cumsum(ar_varianza)

np.where((ar_Pca_suma>0.8) & (ar_Pca_suma<0.9))

"""#### En este caso, con 3 componente tenemos más del 80% de la varianza y con 4 componentes tenemos más del 90%, para este caso seleccionamos 4 componentes."""

pca_v2=PCA(n_components=4)
pca_valor1=pca_v2.fit_transform(std_train)
pca_v2

std_train.shape

pca_valor1.shape

pca_v2.explained_variance_#contienes los valores propios

pca_v2.components_ #contiene los vectores propios

df_pca_loading = pd.DataFrame(pca_valor1, columns=[f'PC{i}' for i in range(1,5)])

df_pca_loading

#Creacion del CSV de datos que entrega el PCA
df_pca_loading.to_csv('df_pca_loading.csv', index=False)

encoder = LabelEncoder()
df['Group'] = encoder.fit_transform(df['Group'])

df['Group']

explained_variance = pca_v2.explained_variance_ratio_
total_explained_variance = sum(explained_variance)
print(f'Varianza explicada por las primeras componentes principales: \n{explained_variance}')
print(f'Varianza explicada acumulada: {total_explained_variance}')

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.scatter(df_pca_loading['PC1'], df_pca_loading['PC2'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC1')
plt.ylabel('PC3')
plt.scatter(df_pca_loading['PC1'], df_pca_loading['PC3'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC1')
plt.ylabel('PC4')
plt.scatter(df_pca_loading['PC1'], df_pca_loading['PC4'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC2')
plt.ylabel('PC3')
plt.scatter(df_pca_loading['PC2'], df_pca_loading['PC3'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC2')
plt.ylabel('PC4')
plt.scatter(df_pca_loading['PC2'], df_pca_loading['PC4'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

# que 'Group' tiene los valores: 1'demented', 2'no demented', 0'converted'

# Definir una paleta de colores personalizada
palette = {
    0: 'yellow',
    1: 'red',
    2: 'green'
}

# Mapear los valores de 'Group' a los colores definidos en la paleta
colors = df['Group'].map(palette)

# Graficar el scatter plot con colores personalizados
plt.figure(figsize=(10, 7))
plt.xlabel('PC3')
plt.ylabel('PC4')
plt.scatter(df_pca_loading['PC3'], df_pca_loading['PC4'], c=colors)
plt.title('PCA Loading Scatter Plot')
plt.show()

"""# Kmeans
## Aplicacion del kmeans para las tres variables del PCA
"""

columna1 = 'PC1'
columna2 = 'PC2'
columna3 = 'PC3'

data = df_pca_loading[[columna1,columna2,columna3]]
data.reset_index(drop=True, inplace=True)

data.head(200)

"""### Cuantos clusters? método Elbow"""

# Encontrar el número de clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(12, 7))
plt.plot(range(1, 11), wcss)
plt.title('Método del codo')
plt.xlabel('Número de clusters')
plt.ylabel('WCSS')
plt.show()

"""## Definicion de clusters K-means"""

import warnings
xn_clusters = 4

warnings.filterwarnings('ignore')
kmeans = KMeans(n_clusters = xn_clusters, init = "k-means++", random_state = 42).fit(data)
data['cluster'] = kmeans.labels_
data.tail()

centers = pd.DataFrame(kmeans.cluster_centers_, columns=['PC1', 'PC2','PC3'])
centers['freq'] = data['cluster'].value_counts()
centers['freq_r'] = data['cluster'].value_counts(normalize=True)
print(centers)

"""## metricas de evaluacion"""

# Muestra el valor de la inertia o WCSS
inertia = kmeans.inertia_
print(f'WCSS: {inertia}')
silhouette_avg = silhouette_score(std_train, kmeans.labels_)
print(f'Coeficiente de silueta promedio: {silhouette_avg}')
centroids = kmeans.cluster_centers_
dist_matrix = cdist(centroids, centroids, 'euclidean')
dist_df = pd.DataFrame(dist_matrix, columns=[f'Cluster {i+1}' for i in range(len(centroids))])
dist_df.index = [f'Cluster {i+1}' for i in range(len(centroids))]

# Mostrar el DataFrame
dist_df

"""## Visualización"""

import seaborn as sns
fig, ax = plt.subplots(figsize=(10, 10))
ax = sns.scatterplot(x=columna1, y=columna2, hue='cluster', style='cluster',
                     ax=ax, data=data)
ax.set_xlim(-5, 6)
ax.set_ylim(-4, 5)
centers.plot.scatter(x=columna1, y=columna2, ax=ax, s=100, color='black')
plt.title('Gráfico Clusters')
plt.tight_layout()
plt.show()

import seaborn as sns
fig, ax = plt.subplots(figsize=(10, 10))
ax = sns.scatterplot(x=columna1, y=columna3, hue='cluster', style='cluster',
                     ax=ax, data=data)
ax.set_xlim(-5, 6)
ax.set_ylim(-4, 5)
centers.plot.scatter(x=columna1, y=columna3, ax=ax, s=100, color='black')
plt.title('Gráfico Clusters')
plt.tight_layout()
plt.show()

import seaborn as sns
fig, ax = plt.subplots(figsize=(10, 10))
ax = sns.scatterplot(x=columna2, y=columna3, hue='cluster', style='cluster',
                     ax=ax, data=data)
ax.set_xlim(-5, 6)
ax.set_ylim(-4, 5)
centers.plot.scatter(x=columna2, y=columna3, ax=ax, s=100, color='black')
plt.title('Gráfico Clusters')
plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 14))
plt.scatter(data[data['cluster']==0][columna1], data[data['cluster']==0][columna2], s = 60, c = 'green', label = 'Cluster1')
plt.scatter(data[data['cluster']==1][columna1], data[data['cluster']==1][columna2], s = 60, c = 'yellow', label = 'Cluster2')
plt.scatter(data[data['cluster']==2][columna1], data[data['cluster']==2][columna2], s = 60, c = 'red', label = 'Cluster3')
plt.scatter(data[data['cluster']==3][columna1], data[data['cluster']==3][columna2], s = 60, c = 'red', label = 'Cluster4')
#plt.scatter(data[data['cluster']==4][columna1], data[data['cluster']==4][columna2], s = 60, c = 'black', label = 'Cluster5')
#plt.scatter(data[data['cluster']==5][columna1], data[data['cluster']==5][columna2], s = 60, c = 'pink', label = 'Cluster6')
plt.scatter(centers[columna1], centers[columna2], s = 100, c = 'black', label = 'Centroides')
plt.title('Gráfico Clusters con centroides')
plt.xlabel(columna1)
plt.ylabel(columna2)
plt.legend()
plt.show()

plt.figure(figsize=(12, 12))
plt.scatter(data[data['cluster']==0][columna1], data[data['cluster']==0][columna2], s = 60, c = 'green', label = 'Cluster1')
plt.scatter(data[data['cluster']==1][columna1], data[data['cluster']==1][columna2], s = 60, c = 'yellow', label = 'Cluster2')
plt.scatter(data[data['cluster']==2][columna1], data[data['cluster']==2][columna2], s = 60, c = 'red', label = 'Cluster3')
plt.scatter(data[data['cluster']==3][columna1], data[data['cluster']==3][columna2], s = 60, c = 'red', label = 'Cluster4')
#plt.scatter(data[data['cluster']==4][columna1], data[data['cluster']==4][columna2], s = 60, c = 'black', label = 'Cluster5')
#plt.scatter(data[data['cluster']==5][columna1], data[data['cluster']==5][columna2], s = 60, c = 'pink', label = 'Cluster6')
#plt.scatter(centers[columna1], centers[columna2], s = 100, c = 'black', label = 'Centroids')
plt.title('Gráfico Clusters')
plt.xlabel(columna1)
plt.ylabel(columna2)
plt.legend()
plt.show()